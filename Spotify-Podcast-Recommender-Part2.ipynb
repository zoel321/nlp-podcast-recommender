{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9ffd5bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ec2e348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5b15362f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fedf9e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "06f56e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.decomposition import NMF, TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c4c2b501",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora, models, matutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "735303a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70bb200",
   "metadata": {},
   "source": [
    "## Cleaning podcast descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5c2fa030",
   "metadata": {},
   "outputs": [],
   "source": [
    "podcast_names_full= pd.read_pickle('just_podcasts.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2e0917b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 350 entries, 0 to 349\n",
      "Data columns (total 3 columns):\n",
      " #   Column               Non-Null Count  Dtype \n",
      "---  ------               --------------  ----- \n",
      " 0   Podcast_Name         350 non-null    object\n",
      " 1   Podcast_ShowID       350 non-null    object\n",
      " 2   Podcast_Description  350 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 8.3+ KB\n"
     ]
    }
   ],
   "source": [
    "#350 podcasts\n",
    "podcast_names_full.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "444c3b53",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Podcast_Name</th>\n",
       "      <th>Podcast_ShowID</th>\n",
       "      <th>Podcast_Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Outer Limits Of Inner Truth</td>\n",
       "      <td>2urV9aIOdLTipAwJ4C3IPu</td>\n",
       "      <td>The Outer Limits of Inner Truth is a program a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>Shit We Don't Tell Mom</td>\n",
       "      <td>26sYAa0ZTK7pbCCOYNYY54</td>\n",
       "      <td>We have depression. Now what? Throw two depres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>Twenty Something Girl</td>\n",
       "      <td>1mt4xNvuz6BVMrD8H9EbpA</td>\n",
       "      <td>Twenty Something Girl is a lifestyle podcast t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>250</th>\n",
       "      <td>Hour of Truth &amp; Power</td>\n",
       "      <td>0W7fw3JvjCHTomEUgx2z6U</td>\n",
       "      <td>Godcast of High Frequency Enlightenment with f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>Donâ€™t Sleep on the Couch Podcast</td>\n",
       "      <td>0xIme1U7WnLvUPsDeocYoh</td>\n",
       "      <td>Formed in 2001, the Don't Sleep on the Couch (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>NAH Podcast</td>\n",
       "      <td>0muSoy4HndaTpELvVDu1iW</td>\n",
       "      <td>Hey Hey! My name is Han or Hannah. Whichever y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Podcast_Name          Podcast_ShowID  \\\n",
       "45        Outer Limits Of Inner Truth  2urV9aIOdLTipAwJ4C3IPu   \n",
       "150            Shit We Don't Tell Mom  26sYAa0ZTK7pbCCOYNYY54   \n",
       "200             Twenty Something Girl  1mt4xNvuz6BVMrD8H9EbpA   \n",
       "250             Hour of Truth & Power  0W7fw3JvjCHTomEUgx2z6U   \n",
       "300  Donâ€™t Sleep on the Couch Podcast  0xIme1U7WnLvUPsDeocYoh   \n",
       "346                       NAH Podcast  0muSoy4HndaTpELvVDu1iW   \n",
       "\n",
       "                                   Podcast_Description  \n",
       "45   The Outer Limits of Inner Truth is a program a...  \n",
       "150  We have depression. Now what? Throw two depres...  \n",
       "200  Twenty Something Girl is a lifestyle podcast t...  \n",
       "250  Godcast of High Frequency Enlightenment with f...  \n",
       "300  Formed in 2001, the Don't Sleep on the Couch (...  \n",
       "346  Hey Hey! My name is Han or Hannah. Whichever y...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "podcast_names_full[podcast_names_full.Podcast_Name.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7620e12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "podcast_names_df = podcast_names_full.drop_duplicates(subset=['Podcast_Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b74450ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "podcast_names_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1cc453",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "868d774c",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e2fdc4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "def clean_regex(series):\n",
    "    # remove digits\n",
    "    desc = series.apply(lambda x: re.sub('\\d', ' ', x ))\n",
    "    # remove \\xa0 from string in Python: https://stackoverflow.com/questions/10993612/how-to-remove-xa0-from-string-in-python\n",
    "    desc = desc.apply(lambda x: x.replace(u'\\xa0', u''))\n",
    "    #remove the | and > symbols\n",
    "    desc = desc.apply(lambda x: re.sub('\\|.+', ' ', x))\n",
    "    desc = desc.apply(lambda x: re.sub('\\>.+', ' ', x))\n",
    "    #remove websites and info that comes after (seems like sponsorship)\n",
    "    desc = desc.apply(lambda x: re.sub('http.+', ' ', x))\n",
    "    desc = desc.apply(lambda x: re.sub('www.+', ' ', x))\n",
    "    #add in space before capital letters if none (some are combined together): referred https://stackoverflow.com/questions/199059/a-pythonic-way-to-insert-a-space-before-capital-letters)\n",
    "    desc = desc.apply(lambda x: re.sub(\"([A-Z])(?![A-Z])\", r\"\\1\", x))\n",
    "    #lowercase\n",
    "    desc=desc.apply(lambda x: x.lower())\n",
    "    #remove punctuation\n",
    "    desc = desc.apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
    "    return desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "43da1f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_pod = clean_regex(podcast_names_df['Podcast_Description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "eecdb52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mwe_tokenizer = MWETokenizer([('personal', 'development'), ('mental', 'health'),('social', 'media'), ('mental', 'illness')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "da1de257",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = cleaned_pod.apply(lambda x: mwe_tokenizer.tokenize(word_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "defcdd0e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [hi, my, name, is, logan, isfeld, i, am, years...\n",
       "1      [being, black, in, has, its, own, challenges, ...\n",
       "2      [the, aubrey, marcus, podcast, is, an, illumin...\n",
       "3      [millions, of, eyes, watching, the, pressure, ...\n",
       "4      [shrugged, collective, is, a, network, of, fit...\n",
       "                             ...                        \n",
       "339    [girls, kickin, up, the, country, is, an, aust...\n",
       "340    [hello, and, welcome, to, happy, and, healthy,...\n",
       "341    [ronald, e, bachman, fsa, maaa, chc, president...\n",
       "342    [atkins, et, al, toward, the, integration, of,...\n",
       "343    [my, name, is, mauricio, perez, vimukta, i, am...\n",
       "Name: Podcast_Description, Length: 344, dtype: object"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c751123d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cleaned_pod_strings = tokenized.apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "814dec14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the podcast for women of color who affirm their worth value mental_health and seek wholeness biweekly mental_health podcast hosted by davia roberts lpc licensed in wi as of october the affirm podcast has discontinued will no longer release episodes thank you for your support'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_pod_strings[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "37d3e6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#helper function to add nouns a second time (upweighing nouns), given string \n",
    "def dup_nouns(string_words):\n",
    "    tokens = pos_tag(word_tokenize(string_words))\n",
    "    li = []\n",
    "    for token in tokens:\n",
    "        li.append(token[0])\n",
    "        if (token[1] == 'NN') or (token[1]=='NNS'):\n",
    "            li.append(token[0])\n",
    "    return li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9fa0cc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns_doubled = cleaned_pod_strings.apply(dup_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "51e1efc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "551f0996",
   "metadata": {},
   "outputs": [],
   "source": [
    "#given list of words, lemmatize words\n",
    "def lem(low):\n",
    "    lemmed = [lemmatizer.lemmatize(word) for word in low]\n",
    "    return lemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "af3de70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_lem = nouns_doubled.apply(lambda x: lem(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c5d4e568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      [hi, hi, my, name, name, is, logan, isfeld, is...\n",
       "1      [being, black, in, ha, it, own, challenge, cha...\n",
       "2      [the, aubrey, marcus, marcus, podcast, podcast...\n",
       "3      [million, million, of, eye, eye, watching, the...\n",
       "4      [shrugged, collective, collective, is, a, netw...\n",
       "                             ...                        \n",
       "339    [girl, girl, kickin, up, the, country, country...\n",
       "340    [hello, hello, and, welcome, welcome, to, happ...\n",
       "341    [ronald, ronald, e, bachman, fsa, fsa, maaa, m...\n",
       "342    [atkins, atkins, et, al, toward, the, integrat...\n",
       "343    [my, name, name, is, mauricio, perez, perez, v...\n",
       "Name: Podcast_Description, Length: 344, dtype: object"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "f7274734",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "default_stop = stopwords.words('english')\n",
    "custom_stop = [\"twitter\", \"instagram\", \"follow\", \"youtube\", \"spotify\", \"check\", 'help', 'ha', 'episode', 'thing', \"like\", \"one\", \"podcast\", \"also\", \"too\", \"much\", \"subscriber\", \"hi\", \"hello\", 'paid', 'week']\n",
    "#my full list of stop words\n",
    "full_list = default_stop + custom_stop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0d476e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/__/04d0437x46368yx2gw6qf2km0000gn/T/ipykernel_3339/2645980770.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  podcast_names_df['Cleaned_Desc'] = tokenized_lem\n"
     ]
    }
   ],
   "source": [
    "podcast_names_df['Cleaned_Desc'] = tokenized_lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "05d52e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = podcast_names_df['Cleaned_Desc'].apply(lambda x: \" \".join(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b8e80d",
   "metadata": {},
   "source": [
    "### Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "51507e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NMF with TF-IDF\n",
    "tfidf_vec = TfidfVectorizer(stop_words=full_list, min_df=2, max_df=0.8, ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "5f685b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_tfidf = tfidf_vec.fit_transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "580119c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abc</th>\n",
       "      <th>abc abc</th>\n",
       "      <th>ability</th>\n",
       "      <th>ability ability</th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>abu</th>\n",
       "      <th>abuse</th>\n",
       "      <th>abuse abuse</th>\n",
       "      <th>abuse mental_health</th>\n",
       "      <th>...</th>\n",
       "      <th>youre thinking</th>\n",
       "      <th>youre youre</th>\n",
       "      <th>youth</th>\n",
       "      <th>youth youth</th>\n",
       "      <th>youve</th>\n",
       "      <th>youve got</th>\n",
       "      <th>youve youve</th>\n",
       "      <th>yr</th>\n",
       "      <th>zu</th>\n",
       "      <th>zu zu</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.308588</td>\n",
       "      <td>0.154294</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.029688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.03894</td>\n",
       "      <td>0.01947</td>\n",
       "      <td>0.022566</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>344 rows Ã— 4275 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     abc  abc abc  ability  ability ability      able  absolutely  abu  \\\n",
       "0    0.0      0.0      0.0              0.0  0.000000         0.0  0.0   \n",
       "1    0.0      0.0      0.0              0.0  0.000000         0.0  0.0   \n",
       "2    0.0      0.0      0.0              0.0  0.000000         0.0  0.0   \n",
       "3    0.0      0.0      0.0              0.0  0.000000         0.0  0.0   \n",
       "4    0.0      0.0      0.0              0.0  0.000000         0.0  0.0   \n",
       "..   ...      ...      ...              ...       ...         ...  ...   \n",
       "339  0.0      0.0      0.0              0.0  0.000000         0.0  0.0   \n",
       "340  0.0      0.0      0.0              0.0  0.029688         0.0  0.0   \n",
       "341  0.0      0.0      0.0              0.0  0.000000         0.0  0.0   \n",
       "342  0.0      0.0      0.0              0.0  0.000000         0.0  0.0   \n",
       "343  0.0      0.0      0.0              0.0  0.000000         0.0  0.0   \n",
       "\n",
       "       abuse  abuse abuse  abuse mental_health  ...  youre thinking  \\\n",
       "0    0.00000      0.00000             0.000000  ...             0.0   \n",
       "1    0.00000      0.00000             0.000000  ...             0.0   \n",
       "2    0.00000      0.00000             0.000000  ...             0.0   \n",
       "3    0.00000      0.00000             0.000000  ...             0.0   \n",
       "4    0.00000      0.00000             0.000000  ...             0.0   \n",
       "..       ...          ...                  ...  ...             ...   \n",
       "339  0.00000      0.00000             0.000000  ...             0.0   \n",
       "340  0.00000      0.00000             0.000000  ...             0.0   \n",
       "341  0.03894      0.01947             0.022566  ...             0.0   \n",
       "342  0.00000      0.00000             0.000000  ...             0.0   \n",
       "343  0.00000      0.00000             0.000000  ...             0.0   \n",
       "\n",
       "     youre youre     youth  youth youth  youve  youve got  youve youve   yr  \\\n",
       "0            0.0  0.308588     0.154294    0.0        0.0          0.0  0.0   \n",
       "1            0.0  0.000000     0.000000    0.0        0.0          0.0  0.0   \n",
       "2            0.0  0.000000     0.000000    0.0        0.0          0.0  0.0   \n",
       "3            0.0  0.000000     0.000000    0.0        0.0          0.0  0.0   \n",
       "4            0.0  0.000000     0.000000    0.0        0.0          0.0  0.0   \n",
       "..           ...       ...          ...    ...        ...          ...  ...   \n",
       "339          0.0  0.000000     0.000000    0.0        0.0          0.0  0.0   \n",
       "340          0.0  0.000000     0.000000    0.0        0.0          0.0  0.0   \n",
       "341          0.0  0.000000     0.000000    0.0        0.0          0.0  0.0   \n",
       "342          0.0  0.000000     0.000000    0.0        0.0          0.0  0.0   \n",
       "343          0.0  0.000000     0.000000    0.0        0.0          0.0  0.0   \n",
       "\n",
       "      zu  zu zu  \n",
       "0    0.0    0.0  \n",
       "1    0.0    0.0  \n",
       "2    0.0    0.0  \n",
       "3    0.0    0.0  \n",
       "4    0.0    0.0  \n",
       "..   ...    ...  \n",
       "339  0.0    0.0  \n",
       "340  0.0    0.0  \n",
       "341  0.0    0.0  \n",
       "342  0.0    0.0  \n",
       "343  0.0    0.0  \n",
       "\n",
       "[344 rows x 4275 columns]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtm_tfidf = pd.DataFrame(doc_term_tfidf.toarray(), columns = tfidf_vec.get_feature_names_out())\n",
    "\n",
    "dtm_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5398ccad",
   "metadata": {},
   "source": [
    "### Topic Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "c462900b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nmf_act = NMF(10, init = 'nndsvda', max_iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "e0dd5ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf = nmf_act.fit(dtm_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "08b44417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to display the top n terms in each topic- sourced from Metis\n",
    "def display_topics(model, feature_names, no_top_words, topic_names = None): \n",
    "    for ix, topic in enumerate(model.components_):\n",
    "        if not topic_names or not topic_names[ix]:\n",
    "            print(\"\\nTopic \", ix + 1)\n",
    "        else:\n",
    "            print(\"\\nTopic: \", topic_names[ix])\n",
    "        print(\", \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "    print(\"\\n\")\n",
    "    return model, feature_names, no_top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "adfba18d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic  1\n",
      "story, guest, story story, show, award, stigma, interview, professional, work, host\n",
      "\n",
      "Topic  2\n",
      "relationship, sex, relationship relationship, life, friendship, friend, welcome, sex sex, career, host\n",
      "\n",
      "Topic  3\n",
      "season, season season, topic, pain, health, science, school, mental_illness, identity, series\n",
      "\n",
      "Topic  4\n",
      "health, fitness, health health, wellness, fitness fitness, show, wellness wellness, nutrition, business, dr\n",
      "\n",
      "Topic  5\n",
      "woman, woman woman, everything, faith, career, body, talk, host, marriage, let\n",
      "\n",
      "Topic  6\n",
      "year, conversation, issue, student, school, education, college, year year, experience, mental_health mental_health\n",
      "\n",
      "Topic  7\n",
      "life, life life, people, share, experience, way, journey, people people, year, challenge\n",
      "\n",
      "Topic  8\n",
      "loss, weight, journey, loss loss, surgery, weight loss, pound, tip, journey journey, bypass\n",
      "\n",
      "Topic  9\n",
      "support, art, support support, culture, art art, people, youd, become, please, art culture\n",
      "\n",
      "Topic  10\n",
      "im, therapy, im im, therapist, therapy therapy, name, die, der, practice, therapist therapist\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# output contents for each topic - TFIDF Vectorizer with NMF\n",
    "\n",
    "output = display_topics(nmf, tfidf_vec.get_feature_names_out(), 10)\n",
    "output;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "75fdfb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ['story', 'relationship', 'season', 'fitness', 'women', 'student','life','weight loss', 'art', 'therapy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "d3c755cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "podcast_doc_topic = nmf.transform(dtm_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "5659e26f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.05869527],\n",
       "       [0.00387598, 0.02641184, 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.00817006, 0.15756237, 0.03108865, ..., 0.00366504, 0.        ,\n",
       "        0.01759739],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.00754332,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.00246465, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.000387  , ..., 0.        , 0.07483999,\n",
       "        0.1006183 ]])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "podcast_doc_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "699e54a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to dataframe\n",
    "podcast_doc_topic_df = pd.DataFrame(podcast_doc_topic.round(3), columns = topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "35c658c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>story</th>\n",
       "      <th>relationship</th>\n",
       "      <th>season</th>\n",
       "      <th>fitness</th>\n",
       "      <th>women</th>\n",
       "      <th>student</th>\n",
       "      <th>life</th>\n",
       "      <th>weight loss</th>\n",
       "      <th>art</th>\n",
       "      <th>therapy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.121</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.004</td>\n",
       "      <td>0.026</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.008</td>\n",
       "      <td>0.158</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.004</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.033</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.229</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.013</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.256</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   story  relationship  season  fitness  women  student   life  weight loss  \\\n",
       "0  0.000         0.000   0.000    0.000  0.000    0.121  0.012        0.000   \n",
       "1  0.004         0.026   0.000    0.000  0.011    0.137  0.009        0.000   \n",
       "2  0.008         0.158   0.031    0.160  0.000    0.022  0.000        0.004   \n",
       "3  0.033         0.003   0.007    0.000  0.000    0.040  0.229        0.005   \n",
       "4  0.013         0.000   0.000    0.256  0.000    0.000  0.000        0.000   \n",
       "\n",
       "   art  therapy  \n",
       "0  0.0    0.059  \n",
       "1  0.0    0.000  \n",
       "2  0.0    0.018  \n",
       "3  0.0    0.000  \n",
       "4  0.0    0.000  "
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "podcast_doc_topic_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d977cd2a",
   "metadata": {},
   "source": [
    "Testing out the recommendation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "39f9813d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = pairwise_distances(np.array(podcast_doc_topic[314]).reshape(1,-1), podcast_doc_topic, metric = 'cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "e9bf9b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([314, 274,   7, 134,   4, 302,  59, 277,  16, 221, 120, 133, 238,\n",
       "       136, 256,  95,  56, 106, 206, 324,  50, 186, 160, 289, 263, 192,\n",
       "        23, 327,  20,  87, 149, 194, 262, 202, 261, 340, 258,  40, 146,\n",
       "        78, 268,  14, 164, 310,   2, 237, 112, 104,  57,  84, 284, 286,\n",
       "       247, 113, 177, 176,  64,  18, 341, 307, 173, 306, 126,  61,   5,\n",
       "       152, 212, 102, 214, 183, 257, 224, 236, 326, 255, 196, 260, 122,\n",
       "       272, 282, 251, 220,  13,  12, 222,  19, 158,  70,  86,  33, 270,\n",
       "       138,  90, 217, 156, 157, 182, 229, 128, 174, 254, 301, 142, 169,\n",
       "       115,  74, 339, 313,  77,  91,   6, 322, 304, 153,  24, 216,  34,\n",
       "        72, 292, 114,  46, 205,  93, 279,  60, 127, 249, 166,  25,  97,\n",
       "       259, 253, 243, 147, 116, 125, 154,  94, 159, 148,  51, 319,  65,\n",
       "       178, 250, 184, 335, 225,  49, 311, 330, 181,  99,  11,  63,  35,\n",
       "       110,  38, 130, 323, 240, 150, 198, 232,  69, 187, 231,  53, 223,\n",
       "       234,  39, 185,  73, 318,  98,  83,  32, 213, 271, 190, 208,  80,\n",
       "       264, 163, 230, 332,  10, 328, 175, 144, 241, 235, 167, 283, 269,\n",
       "       293, 312,  75,   3, 239, 204, 135,   9, 105, 316,  68,  76,  43,\n",
       "       168, 210, 280, 124, 180,  92, 197, 123,  37,  54, 189,  41, 321,\n",
       "       211, 305,   8, 131, 299, 193, 291, 161,  22,  85, 298,  31, 209,\n",
       "       199, 309,  89, 228, 109, 252,  36, 155,  26, 218, 203, 118, 296,\n",
       "       171, 139,  52, 143, 101,  82,  79, 320, 317, 297, 242, 308, 165,\n",
       "       108, 333,  44, 172,  58, 290, 246,  71,  29,  17, 315, 337, 100,\n",
       "       288, 275, 265, 121, 200, 226,  42, 141, 245,  28, 219, 285, 300,\n",
       "       233,  96, 107, 303, 140, 266,  66,   1, 215, 119,  88, 278, 244,\n",
       "       132, 343, 137,  67, 145, 151, 325, 227, 338,  81,  45, 294, 336,\n",
       "       188, 103,   0, 170, 331, 287, 201,  30, 273, 129, 329,  15,  21,\n",
       "        27, 281, 276, 334,  47,  48, 342, 179,  62, 267, 248, 207, 111,\n",
       "       295, 195, 191, 162,  55, 117])"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.argsort()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "a0d2fe38",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([274,   7, 134,   4, 302,  59, 277,  16, 221])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.argsort()[0][1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "c6e913c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Glen Alex is Author of Living In Total Health, the 2021 Indie Book Award Winner for Health/Wellness and Finalist in the Mind, Body, Spirit category, a Clinical Social Worker, and the Wellth Coach.The Glen Alex Show is all about health! Each episode focuses on a specific area of your health, including physical, mental, emotional, and spiritual health. We focus on the whole person because all of you matter. Glen Alex and her guests provide valuable information and insights to help you be joyful, connected, confident, and complete--our mission!About Glen AlexGlen Alex, author of Living in Total Health, has a mission to help people be more joyful, connected, confident and complete. This is a life experience she refers to as Wellth: Health + other riches in life. Glen is a Licensed Clinical Social Worker who delivers counseling and coaching services and is the guiding spirit behind The Glen Alex Show, GlenAlex.com, Healthy Boundaries for Overwhelmed Women online course, and other services in fulfillment of her transformative mission. You can contact Glen Alex at Glen@GlenAlex.com or (702) 766-6492.'"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "podcast_names_df.Podcast_Description[314]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "50cccf9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Joytme Fitness presents Be Extraordinary Podcast. Tune in to hear conversations with leaders and influencers that will inspire you to reach your full potential. The show has two HOST \"Joetta\" 1988, 1992, 1996, 2000 Olympian, Motivator, Author and Leading Authority on Health/Wellness and Achievement. \" Tyrone\" Retired Firefighter, Sports/Mental Coach and Purpose Master Motivator.'"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "podcast_names_df.Podcast_Description[274]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "4df74382",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Your Mind Matters podcast is a weekly show where we chat all things mental wellness, lifestyle, and I share my stories while answering listener questions. Hosted by Natalie Bally, a health and fitness content creator who has been sharing her journey since 2018 on both Instagram and YouTube. My favorite thing to do is talk about all the random *and sometimes insightful* thoughts in my head so join me weekly to hear whatâ€™s going on up there ðŸ™ƒ'"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "podcast_names_df.Podcast_Description[7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c4080512",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pickling both the unique podcasts dataframe and doc_topic dataframe \n",
    "podcast_names_df.to_pickle('unique_podcastnames.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "9f963f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "podcast_doc_topic_df.to_pickle('podcast_doc_topic.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768d2c25",
   "metadata": {},
   "source": [
    "## Recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "69066460",
   "metadata": {},
   "outputs": [],
   "source": [
    "ep_doc_topic = pd.read_pickle('doc_topic2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "6f9a32ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_df = pd.read_pickle('mh_podcasts_unique.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "2acb8411",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Podcast_Name</th>\n",
       "      <th>Podcast_ShowID</th>\n",
       "      <th>Podcast_Description</th>\n",
       "      <th>Cleaned_Desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>(2020) Mental Health Explained | Created By Yo...</td>\n",
       "      <td>4pwPCZriBVbcLcufvtchsP</td>\n",
       "      <td>Hi, my name is Logan Isfeld, I am 17 years old...</td>\n",
       "      <td>[hi, hi, my, name, name, is, logan, isfeld, is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Being African American in 2021 and dealing wit...</td>\n",
       "      <td>4eoXzwruqyu2yAh4jYA7EM</td>\n",
       "      <td>Being black in 2021 has its own challenges and...</td>\n",
       "      <td>[being, black, in, ha, it, own, challenge, cha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Aubrey Marcus Podcast</td>\n",
       "      <td>0n7j2qseg6fu0Fj2dvzXVi</td>\n",
       "      <td>The Aubrey Marcus Podcast is an illuminating c...</td>\n",
       "      <td>[the, aubrey, marcus, marcus, podcast, podcast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Unfazed and Unbothered with Tasia and Camo</td>\n",
       "      <td>6MZJi1fkxSbqjfQiSqC5OL</td>\n",
       "      <td>Millions of eyes watching, the pressure, the n...</td>\n",
       "      <td>[million, million, of, eye, eye, watching, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Barbell Shrugged</td>\n",
       "      <td>6MFeb0x9bw9wjrphztLSn9</td>\n",
       "      <td>Shrugged Collective is a network of fitness, h...</td>\n",
       "      <td>[shrugged, collective, collective, is, a, netw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>344</td>\n",
       "      <td>Welcome to GKUTC</td>\n",
       "      <td>4kvLOHbayUXH6QZBPQ2OPV</td>\n",
       "      <td>Girls Kickin Up The Country is an Australian A...</td>\n",
       "      <td>[girl, girl, kickin, up, the, country, country...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>340</th>\n",
       "      <td>345</td>\n",
       "      <td>Happy and Healthy Mind with Dr. Rozina</td>\n",
       "      <td>5XwuvVKnlVtKNBluBl0ITY</td>\n",
       "      <td>Hello and welcome to Happy and Healthy mind wi...</td>\n",
       "      <td>[hello, hello, and, welcome, welcome, to, happ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>347</td>\n",
       "      <td>Healthcare Insight</td>\n",
       "      <td>5GO3DnQpENyNVJymwG8BjU</td>\n",
       "      <td>Ronald E. Bachman FSA, MAAA, CHC  President &amp; ...</td>\n",
       "      <td>[ronald, ronald, e, bachman, fsa, fsa, maaa, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>348</td>\n",
       "      <td>Mental Health Education in High Schools</td>\n",
       "      <td>2Ow2pcCGA3rcRDVxSjhI6C</td>\n",
       "      <td>Atkins et al. (2010). Toward the integration o...</td>\n",
       "      <td>[atkins, atkins, et, al, toward, the, integrat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>349</td>\n",
       "      <td>Prabhat Ranjan Sarkar Discourses</td>\n",
       "      <td>3iV30kXhmDEJ5Ed1gEFVSm</td>\n",
       "      <td>My name is Mauricio Perez (Vimukta), I am a So...</td>\n",
       "      <td>[my, name, name, is, mauricio, perez, perez, v...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>344 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index                                       Podcast_Name  \\\n",
       "0        0  (2020) Mental Health Explained | Created By Yo...   \n",
       "1        1  Being African American in 2021 and dealing wit...   \n",
       "2        2                              Aubrey Marcus Podcast   \n",
       "3        3         Unfazed and Unbothered with Tasia and Camo   \n",
       "4        4                                   Barbell Shrugged   \n",
       "..     ...                                                ...   \n",
       "339    344                                  Welcome to GKUTC    \n",
       "340    345             Happy and Healthy Mind with Dr. Rozina   \n",
       "341    347                                 Healthcare Insight   \n",
       "342    348            Mental Health Education in High Schools   \n",
       "343    349                   Prabhat Ranjan Sarkar Discourses   \n",
       "\n",
       "             Podcast_ShowID  \\\n",
       "0    4pwPCZriBVbcLcufvtchsP   \n",
       "1    4eoXzwruqyu2yAh4jYA7EM   \n",
       "2    0n7j2qseg6fu0Fj2dvzXVi   \n",
       "3    6MZJi1fkxSbqjfQiSqC5OL   \n",
       "4    6MFeb0x9bw9wjrphztLSn9   \n",
       "..                      ...   \n",
       "339  4kvLOHbayUXH6QZBPQ2OPV   \n",
       "340  5XwuvVKnlVtKNBluBl0ITY   \n",
       "341  5GO3DnQpENyNVJymwG8BjU   \n",
       "342  2Ow2pcCGA3rcRDVxSjhI6C   \n",
       "343  3iV30kXhmDEJ5Ed1gEFVSm   \n",
       "\n",
       "                                   Podcast_Description  \\\n",
       "0    Hi, my name is Logan Isfeld, I am 17 years old...   \n",
       "1    Being black in 2021 has its own challenges and...   \n",
       "2    The Aubrey Marcus Podcast is an illuminating c...   \n",
       "3    Millions of eyes watching, the pressure, the n...   \n",
       "4    Shrugged Collective is a network of fitness, h...   \n",
       "..                                                 ...   \n",
       "339  Girls Kickin Up The Country is an Australian A...   \n",
       "340  Hello and welcome to Happy and Healthy mind wi...   \n",
       "341  Ronald E. Bachman FSA, MAAA, CHC  President & ...   \n",
       "342  Atkins et al. (2010). Toward the integration o...   \n",
       "343  My name is Mauricio Perez (Vimukta), I am a So...   \n",
       "\n",
       "                                          Cleaned_Desc  \n",
       "0    [hi, hi, my, name, name, is, logan, isfeld, is...  \n",
       "1    [being, black, in, ha, it, own, challenge, cha...  \n",
       "2    [the, aubrey, marcus, marcus, podcast, podcast...  \n",
       "3    [million, million, of, eye, eye, watching, the...  \n",
       "4    [shrugged, collective, collective, is, a, netw...  \n",
       "..                                                 ...  \n",
       "339  [girl, girl, kickin, up, the, country, country...  \n",
       "340  [hello, hello, and, welcome, welcome, to, happ...  \n",
       "341  [ronald, ronald, e, bachman, fsa, fsa, maaa, m...  \n",
       "342  [atkins, atkins, et, al, toward, the, integrat...  \n",
       "343  [my, name, name, is, mauricio, perez, perez, v...  \n",
       "\n",
       "[344 rows x 5 columns]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "podcast_names_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "1c6046fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Podcast_Name</th>\n",
       "      <th>Podcast_ShowID</th>\n",
       "      <th>Podcast_Description</th>\n",
       "      <th>Cleaned_Desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>341</th>\n",
       "      <td>347</td>\n",
       "      <td>Healthcare Insight</td>\n",
       "      <td>5GO3DnQpENyNVJymwG8BjU</td>\n",
       "      <td>Ronald E. Bachman FSA, MAAA, CHC  President &amp; ...</td>\n",
       "      <td>[ronald, ronald, e, bachman, fsa, fsa, maaa, m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     index        Podcast_Name          Podcast_ShowID  \\\n",
       "341    347  Healthcare Insight  5GO3DnQpENyNVJymwG8BjU   \n",
       "\n",
       "                                   Podcast_Description  \\\n",
       "341  Ronald E. Bachman FSA, MAAA, CHC  President & ...   \n",
       "\n",
       "                                          Cleaned_Desc  \n",
       "341  [ronald, ronald, e, bachman, fsa, fsa, maaa, m...  "
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "podcast_names_df[podcast_names_df[\"Podcast_Name\"] == 'Healthcare Insight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "cf09eb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = podcast_names_df[podcast_names_df[\"Podcast_Name\"] == 'Barbell Shrugged'].index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "8fa9bf64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "aae3df58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_podcast(podcast_name, pod_df, pod_doc_topic):\n",
    "    podcast_row = pod_df[pod_df['Podcast_Name'] == podcast_name]\n",
    "    podcast_index = podcast_row.index[0]\n",
    "    #print('Given: ', podcast_name)\n",
    "    #print('Description: ', pod_df.iloc[podcast_index]['Podcast_Description'])\n",
    "    dist = pairwise_distances(np.array(pod_doc_topic.iloc[podcast_index]).reshape(1,-1), pod_doc_topic, metric = 'cosine')\n",
    "    rec_pod_index = dist.argsort()[0][1]\n",
    "    rec_pod_name = pod_df.iloc[rec_pod_index]['Podcast_Name']\n",
    "    #print('\\nRecommended Podcast: ', rec_pod_name)\n",
    "    #print('Description: ', pod_df.iloc[rec_pod_index]['Podcast_Description'])\n",
    "    return rec_pod_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "7da5f2be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Be Extraordinary  Joetta Clark Olympian/Motivator & Tyrone Retired Firefighter/ Mental Coach '"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_podcast('Barbell Shrugged', podcast_names_df, podcast_doc_topic_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "09e038b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_episode(ep_index, ep_df, ep_doc_topic, shortened_doc_topic=None):\n",
    "    if shortened_doc_topic is not None:\n",
    "        dist = pairwise_distances(np.array(ep_doc_topic.iloc[ep_index]).reshape(1,-1), shortened_doc_topic, metric = 'cosine')\n",
    "        rec_ep_index = dist.argsort()[0][1]\n",
    "        rec_ep_name = ep_df.reset_index().iloc[rec_ep_index]['Ep_name']\n",
    "    else: \n",
    "        dist = pairwise_distances(np.array(ep_doc_topic.iloc[ep_index]).reshape(1,-1), ep_doc_topic, metric = 'cosine')\n",
    "        rec_ep_index = dist.argsort()[0][1]\n",
    "        rec_ep_name = ep_df.iloc[rec_ep_index]['Ep_name']\n",
    "    return rec_ep_index, rec_ep_name "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "fca727e6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14560, 'Coming To Terms')"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommend_episode(250, episode_df, ep_doc_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "3c7f719b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# episode_df[episode_df.Ep_name.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "id": "4901385d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Enjoy our conversation with my guests today while we talk about motivation, in higher education and higher Fitness Levels.\\xa0 Both of my guests are inspiring-wives, mothers, entrepreneurs and educators. Lina Mendez is an Associate Director of ChicanX & Latinx Retention Initiatives at UC Davis. She holds a BA degree in education from New Mexico State, Masters from Harvard, and PhD from UC Davis. Find her @linarmendez for Twitter. \\xa0Nicole Sims is an International Federation of BodyBuilding and fitness Professional. She received her BA in Communications from Washington State. She has her own business as a Health and Accountability Coach in the Atlanta, Georgia area. Find her on Instagram @nicolesimswellness. For suggestions: website: www.fuertefitness.com\\xa0 mail us at: fuertefitness@gmail.com Facebook page: https://www.facebook.com/fuertefitness/Instagram: @fuertefitness and @funkiecoldmedina'"
      ]
     },
     "execution_count": 609,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "episode_df.iloc[16820]['Ep_desc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "ac4561b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rec_ep_diff_podcast(ep_index, pod_index, ep_df, ep_doc_topic, pod_doc_topic):\n",
    "    pod_dist = pairwise_distances(np.array(pod_doc_topic.iloc[pod_index]).reshape(1,-1), pod_doc_topic, metric = 'cosine')\n",
    "    ep_dist =  pairwise_distances(np.array(ep_doc_topic.iloc[ep_index]).reshape(1,-1), ep_doc_topic, metric = 'cosine')\n",
    "    #making dataframe to organize summed cosine similarity\n",
    "    start_df = ep_df['Podcast_Name'].to_frame()\n",
    "    start_df['Pod_Index'] = start_df['Podcast_Name'].map(pod_dict).str[0]\n",
    "    ep_cos_df = pd.DataFrame(ep_dist.reshape(-1,1), columns=['Cosine Similarity for Episodes'])\n",
    "    full = start_df.join(ep_cos_df)\n",
    "    full['Cosine Similarity for Podcasts']= full['Pod_Index'].apply(lambda x: pod_dist[0][x])\n",
    "    full['Summed Similarity'] = full[\"Cosine Similarity for Episodes\"] + full[\"Cosine Similarity for Podcasts\"]\n",
    "    #find row for smallest summed similarity, excluding rows with same podcast name\n",
    "    rec_ep_index = full[full['Pod_Index']!= pod_index].sort_values(by=['Summed Similarity']).index[0]\n",
    "    return rec_ep_index\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "9d3fb4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for Different podcast: first finds podcast rec then closest cosine similarity from there\n",
    "def recommender(ep_index, ep_df, pod_df, ep_doc_topic, pod_doc_topic, podcast_choice):\n",
    "    episode_name = ep_df.iloc[ep_index]['Ep_name']\n",
    "    print('Episode Name: ', episode_name)\n",
    "    print('Episode Date: ', ep_df.iloc[ep_index]['Ep_date'])\n",
    "    #ep_row = ep_df[ep_df['Ep_Name'] == episode_name]\n",
    "    #ep_index = ep_row.index[0]\n",
    "    podcast_name = ep_df.iloc[ep_index]['Podcast_Name']\n",
    "    print('From podcast: ', podcast_name)\n",
    "    print('Description: ', ep_df.iloc[ep_index]['Ep_desc'])\n",
    "    if podcast_choice == 'Different':\n",
    "        rec_pod_name = recommend_podcast(podcast_name, pod_df, pod_doc_topic)\n",
    "        print(rec_pod_name)\n",
    "        limited_indices = ep_df.index[ep_df['Podcast_Name'] == rec_pod_name].tolist()\n",
    "        limited_ep_df = ep_df.iloc[limited_indices]\n",
    "        limited_doc_topic = ep_doc_topic.iloc[limited_indices]\n",
    "        rec_ep_index, _ = recommend_episode(ep_index, limited_ep_df, ep_doc_topic, shortened_doc_topic=limited_doc_topic)\n",
    "        #updating these because returned indices are from shortened array\n",
    "        rec_ep_name = limited_ep_df.reset_index().iloc[rec_ep_index]['Ep_name']\n",
    "        rec_ep_index = limited_ep_df.reset_index().iloc[rec_ep_index]['level_0']\n",
    "        #print(rec_ep_index, rec_ep_name)\n",
    "    if podcast_choice == 'Same':\n",
    "        limited_indices = ep_df.index[ep_df['Podcast_Name'] == podcast_name].tolist()\n",
    "        limited_ep_df = ep_df.iloc[limited_indices]\n",
    "        limited_doc_topic = ep_doc_topic.iloc[limited_indices]\n",
    "        rec_ep_index, _ = recommend_episode(ep_index, limited_ep_df, ep_doc_topic, shortened_doc_topic= limited_doc_topic)\n",
    "        #updating these because returned indices are from shortened array\n",
    "        rec_ep_name = limited_ep_df.reset_index().iloc[rec_ep_index]['Ep_name']\n",
    "        rec_ep_index = limited_ep_df.reset_index().iloc[rec_ep_index]['level_0']\n",
    "    if podcast_choice == 'Any':\n",
    "        rec_ep_index, rec_ep_name = recommend_episode(ep_index, ep_df, ep_doc_topic)\n",
    "    print('Recommended episode: ', rec_ep_name)\n",
    "    rec_ep_info = ep_df.iloc[rec_ep_index]\n",
    "    print('From podcast: ', rec_ep_info['Podcast_Name'])\n",
    "    print('Date: ', rec_ep_info['Ep_date'])\n",
    "    print('Description: ', rec_ep_info['Ep_desc'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "8110a1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary of podcast names to indices\n",
    "pod_dict = podcast_names_df.groupby('Podcast_Name').indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "c877b4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "##for Different podcast: this does not filter first, but takes everything (both doc-topics) as whole \n",
    "\n",
    "def recommender2(ep_index, ep_df, pod_df, ep_doc_topic, pod_doc_topic, podcast_choice):\n",
    "    episode_name = ep_df.iloc[ep_index]['Ep_name']\n",
    "    print('Episode Name: ', episode_name)\n",
    "    print('Episode Date: ', ep_df.iloc[ep_index]['Ep_date'])\n",
    "    #ep_row = ep_df[ep_df['Ep_Name'] == episode_name]\n",
    "    #ep_index = ep_row.index[0]\n",
    "    podcast_name = ep_df.iloc[ep_index]['Podcast_Name']\n",
    "    print('From podcast: ', podcast_name)\n",
    "    print('Description: ', ep_df.iloc[ep_index]['Ep_desc'])\n",
    "    if podcast_choice == 'Different':\n",
    "        podcast_index = pod_df[pod_df[\"Podcast_Name\"]== podcast_name].index[0]\n",
    "        rec_ep_index = rec_ep_diff_podcast(ep_index, podcast_index, ep_df, ep_doc_topic, pod_doc_topic)\n",
    "        rec_ep_name = ep_df.iloc[rec_ep_index]['Ep_name']\n",
    "    if podcast_choice == 'Same':\n",
    "        limited_indices = ep_df.index[ep_df['Podcast_Name'] == podcast_name].tolist()\n",
    "        limited_ep_df = ep_df.iloc[limited_indices]\n",
    "        limited_doc_topic = ep_doc_topic.iloc[limited_indices]\n",
    "        rec_ep_index, _ = recommend_episode(ep_index, limited_ep_df, ep_doc_topic, shortened_doc_topic= limited_doc_topic)\n",
    "        #updating these because returned indices are from shortened array\n",
    "        rec_ep_name = limited_ep_df.reset_index().iloc[rec_ep_index]['Ep_name']\n",
    "        rec_ep_index = limited_ep_df.reset_index().iloc[rec_ep_index]['level_0']\n",
    "    if podcast_choice == 'Any':\n",
    "        rec_ep_index, rec_ep_name = recommend_episode(ep_index, ep_df, ep_doc_topic)\n",
    "    print('Recommended episode: ', rec_ep_name)\n",
    "    rec_ep_info = ep_df.iloc[rec_ep_index]\n",
    "    print('From podcast: ', rec_ep_info['Podcast_Name'])\n",
    "    print('Date: ', rec_ep_info['Ep_date'])\n",
    "    print('Description: ', rec_ep_info['Ep_desc'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "45dbbbb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Name:  Strength PHD: The Simplified Equations of Strength and Conditioning w/ Anders Varner, Doug Larson, and Travis Mash  - Barbell Shrugged #526\n",
      "Episode Date:  2020-11-30\n",
      "From podcast:  Barbell Shrugged\n",
      "Description:  Buy Strength PHD and help support weightlifters at Lenoir-Rhyne University Â  In this Episode of Barbell Shrugged:  Why every coach needs to understand the fundamental equations of strength How to incorporate these equations into a training program. How to master energy systemsÂ  What is impulse and how to improve it Nutrition for hypertrophy  Buy Strength PHD and help support weightlifters at Lenoir-Rhyne University Â  Anders Varner on Instagram Doug Larson on Instagram Coach Travis Mash on Instagram â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” Training Programs to Build Muscle: https://bit.ly/34zcGVw Â  Nutrition Programs to Lose Fat and Build Muscle: https://bit.ly/3eiW8FF Â  Nutrition and Training Bundles to Save 67%: https://bit.ly/2yaxQxa Â  Please Support Our Sponsors Â  PowerDot - Save 20% using code BBS at http://PowerDot.com/BBSÂ  Â  Inside Tracker: insidetracker.com/earlyaccess to be the first to hear about InsideTrackerâ€™s BEST DEAL of the yearÂ  Â  Fittogether - Fitness ONLY Social Media App Â  Organifi - Save 20% using code: â€œShruggedâ€ at organifi.com/shrugged Â  www.masszymes.com/shruggedfree Â - for FREE bottle of BiOptimizers Masszymes Â  Garage Gym Equipment and Accessories: https://bit.ly/3b6GZFj Save 5% using the coupon code â€œShruggedâ€\n",
      "Recommended episode:  What keeps you Motivated?\n",
      "From podcast:  Being Fuerte. Itâ€™s Time to Speak!\n",
      "Date:  2021-02-14\n",
      "Description:  Enjoy our conversation with my guests today while we talk about motivation, in higher education and higher Fitness Levels.Â  Both of my guests are inspiring-wives, mothers, entrepreneurs and educators. Lina Mendez is an Associate Director of ChicanX & Latinx Retention Initiatives at UC Davis. She holds a BA degree in education from New Mexico State, Masters from Harvard, and PhD from UC Davis. Find her @linarmendez for Twitter. Â Nicole Sims is an International Federation of BodyBuilding and fitness Professional. She received her BA in Communications from Washington State. She has her own business as a Health and Accountability Coach in the Atlanta, Georgia area. Find her on Instagram @nicolesimswellness. For suggestions: website: www.fuertefitness.comÂ  mail us at: fuertefitness@gmail.com Facebook page: https://www.facebook.com/fuertefitness/Instagram: @fuertefitness and @funkiecoldmedina\n"
     ]
    }
   ],
   "source": [
    "recommender2(500, episode_df, podcast_names_df, ep_doc_topic, podcast_doc_topic_df, podcast_choice='Different')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed155f82",
   "metadata": {},
   "source": [
    "#### Supplemental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "id": "a51ffc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = episode_df['Podcast_Name'].to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "id": "3a25668f",
   "metadata": {},
   "outputs": [],
   "source": [
    "p['Pod_Index'] = p['Podcast_Name'].map(pod_dict).str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "id": "5d482238",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(ep_dist.reshape(-1,1), columns=['Cosine Similarity for Episodes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "id": "18336930",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df2 = p.join(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "id": "5e13acea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Cosine Similarity for Podcasts']= df2['Pod_Index'].apply(lambda x: pod_dist[0][x] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "id": "6cdfaaf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Summed Similarity'] = df2[\"Cosine Similarity for Episodes\"] + df2[\"Cosine Similarity for Podcasts\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "id": "2ac03aae",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Podcast_Name</th>\n",
       "      <th>Pod_Index</th>\n",
       "      <th>Cosine Similarity for Episodes</th>\n",
       "      <th>Cosine Similarity for Podcasts</th>\n",
       "      <th>Summed Similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16820</th>\n",
       "      <td>Being Fuerte. Itâ€™s Time to Speak!</td>\n",
       "      <td>302</td>\n",
       "      <td>0.137119</td>\n",
       "      <td>0.019527</td>\n",
       "      <td>0.156645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10286</th>\n",
       "      <td>Non Gendered Fitness</td>\n",
       "      <td>120</td>\n",
       "      <td>0.153246</td>\n",
       "      <td>0.010033</td>\n",
       "      <td>0.163280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10260</th>\n",
       "      <td>Non Gendered Fitness</td>\n",
       "      <td>120</td>\n",
       "      <td>0.170357</td>\n",
       "      <td>0.010033</td>\n",
       "      <td>0.180390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9208</th>\n",
       "      <td>The Remote CEO Show</td>\n",
       "      <td>95</td>\n",
       "      <td>0.034548</td>\n",
       "      <td>0.175606</td>\n",
       "      <td>0.210154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10298</th>\n",
       "      <td>Non Gendered Fitness</td>\n",
       "      <td>120</td>\n",
       "      <td>0.204449</td>\n",
       "      <td>0.010033</td>\n",
       "      <td>0.214482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8326</th>\n",
       "      <td>Moments of Clarity with Tiffany</td>\n",
       "      <td>86</td>\n",
       "      <td>1.549724</td>\n",
       "      <td>0.781614</td>\n",
       "      <td>2.331338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8407</th>\n",
       "      <td>Moments of Clarity with Tiffany</td>\n",
       "      <td>86</td>\n",
       "      <td>1.549724</td>\n",
       "      <td>0.781614</td>\n",
       "      <td>2.331338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8626</th>\n",
       "      <td>Moments of Clarity with Tiffany</td>\n",
       "      <td>86</td>\n",
       "      <td>1.549724</td>\n",
       "      <td>0.781614</td>\n",
       "      <td>2.331338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8659</th>\n",
       "      <td>Moments of Clarity with Tiffany</td>\n",
       "      <td>86</td>\n",
       "      <td>1.549724</td>\n",
       "      <td>0.781614</td>\n",
       "      <td>2.331338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17142</th>\n",
       "      <td>She Just Went There</td>\n",
       "      <td>317</td>\n",
       "      <td>1.355328</td>\n",
       "      <td>0.997406</td>\n",
       "      <td>2.352733</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17262 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Podcast_Name  Pod_Index  \\\n",
       "16820  Being Fuerte. Itâ€™s Time to Speak!        302   \n",
       "10286               Non Gendered Fitness        120   \n",
       "10260               Non Gendered Fitness        120   \n",
       "9208                 The Remote CEO Show         95   \n",
       "10298               Non Gendered Fitness        120   \n",
       "...                                  ...        ...   \n",
       "8326     Moments of Clarity with Tiffany         86   \n",
       "8407     Moments of Clarity with Tiffany         86   \n",
       "8626     Moments of Clarity with Tiffany         86   \n",
       "8659     Moments of Clarity with Tiffany         86   \n",
       "17142                She Just Went There        317   \n",
       "\n",
       "       Cosine Similarity for Episodes  Cosine Similarity for Podcasts  \\\n",
       "16820                        0.137119                        0.019527   \n",
       "10286                        0.153246                        0.010033   \n",
       "10260                        0.170357                        0.010033   \n",
       "9208                         0.034548                        0.175606   \n",
       "10298                        0.204449                        0.010033   \n",
       "...                               ...                             ...   \n",
       "8326                         1.549724                        0.781614   \n",
       "8407                         1.549724                        0.781614   \n",
       "8626                         1.549724                        0.781614   \n",
       "8659                         1.549724                        0.781614   \n",
       "17142                        1.355328                        0.997406   \n",
       "\n",
       "       Summed Similarity  \n",
       "16820           0.156645  \n",
       "10286           0.163280  \n",
       "10260           0.180390  \n",
       "9208            0.210154  \n",
       "10298           0.214482  \n",
       "...                  ...  \n",
       "8326            2.331338  \n",
       "8407            2.331338  \n",
       "8626            2.331338  \n",
       "8659            2.331338  \n",
       "17142           2.352733  \n",
       "\n",
       "[17262 rows x 5 columns]"
      ]
     },
     "execution_count": 617,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[df2['Pod_Index']!= 4].sort_values(by=['Summed Similarity'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
